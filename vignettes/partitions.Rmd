---
title: "Working with Partitioned Datasets"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Working with Partitioned Datasets}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

if (requireNamespace("pkgload", quietly = TRUE)) {
  pkgload::load_all(".")
} else {
  library(stamp)
}

library(data.table)
```

This vignette demonstrates stamp's partitioning system for managing large datasets split across multiple files using Hive-style directory structures. Partitioning is ideal for:

- **Large datasets** that need to be split by country, year, region, etc.
- **Selective loading** where you only need specific subsets (e.g., one country's data)
- **Columnar efficiency** when using parquet/fst formats (load only needed columns)
- **Parallel processing** where partitions can be processed independently

## Quick Start

```{r quickstart}
# Initialize stamp
tdir <- tempfile("stamp-partitions-")
dir.create(tdir)
st_init(tdir)

# Create sample welfare data
set.seed(123)  # for reproducible vignette output
welfare <- data.table(
  country = rep(c("USA", "CAN", "MEX"), each = 100),
  year = rep(2020:2024, 60),
  reporting_level = sample(
    c("national", "urban", "rural"),
    300,
    replace = TRUE
  ),
  hh_id = 1:300,
  income = rnorm(300, 50000, 15000),
  consumption = rnorm(300, 35000, 10000)
)

# Auto-partition and save (eliminates manual looping!)
parts_dir <- file.path(tdir, "welfare_parts")

manifest <- st_write_parts(
  welfare,
  base = parts_dir,
  partitioning = c("country", "year", "reporting_level"),
  code_label = "welfare_data"
)

# View manifest
head(manifest, 3)
```

The manifest shows:
- `partition_key`: List of key-value pairs for each partition
- `path`: File location with Hive-style structure
- `version_id`: Version identifier for tracking
- `n_rows`: Number of rows in partition

## Hive-Style Partitioning

Partitions are organized in directories following the Hive convention:

```
base_dir/
  key1=value1/
    key2=value2/
      part.parquet
```

For example:
```
welfare_parts/
  country=USA/
    year=2020/
      reporting_level=national/
        part.parquet
      reporting_level=urban/
        part.parquet
```

This structure:
- **Self-documenting**: Directory names encode partition values
- **Efficient discovery**: Tools can scan directory structure to find partitions
- **Standard format**: Compatible with Apache Spark, Apache Arrow, DuckDB, etc.

## Auto-Partitioning with `st_write_parts()`

Before `st_write_parts()`, you had to manually loop through partition combinations:

```{r old-way, eval=FALSE}
# ❌ OLD WAY - Manual looping (verbose, error-prone)
for (ctry in unique(welfare$country)) {
  for (yr in unique(welfare$year)) {
    for (rl in unique(welfare$reporting_level)) {
      part_dt <- welfare[country == ctry & year == yr & reporting_level == rl]
      if (nrow(part_dt) > 0) {
        st_save_part(
          part_dt,
          base = parts_dir,
          key = list(country = ctry, year = yr, reporting_level = rl)
        )
      }
    }
  }
}
```

Now it's a single function call:

```{r new-way}
# ✅ NEW WAY - Auto-partitioning (clean, efficient)
manifest <- st_write_parts(
  welfare,
  base = parts_dir,
  partitioning = c("country", "year", "reporting_level"),
  code_label = "welfare_data",
  .progress = FALSE # Disable progress bar for vignette
)
  
cat(sprintf("Saved %d partitions\n", nrow(manifest)))
```

### Benefits

1. **Automatic splitting**: Handles all combinations of partition values
2. **Progress tracking**: Shows progress bar for many partitions (>10 by default)
3. **Error handling**: Gracefully handles failed partitions
4. **Consistent metadata**: All partitions get same code_label, versioning
5. **Performance**: Uses `data.table::split()` when available for speed

### Format Selection

By default, `st_write_parts()` uses **parquet** format for optimal columnar performance:

```{r format-default}
# Default format is parquet
format_manifest <- st_write_parts(
  welfare[1:50],
  base = file.path(tdir, "test_format"),
  partitioning = "country",
  .progress = FALSE
)

# Check file extension
basename(format_manifest$path[1])
```

Override with explicit format:

```{r format-override}
# Use fst format instead
manifest_fst <- st_write_parts(
  welfare[1:50],
  base = file.path(tdir, "test_fst"),
  partitioning = "country",
  format = "fst",
  .progress = FALSE
)

basename(manifest_fst$path[1])
```

## Loading Partitions

### Load All Partitions

```{r load-all}
# Load all partitions and row-bind
all_data <- st_load_parts(parts_dir, as = "dt")

cat(sprintf(
  "Loaded %d rows from %d partitions\n",
  nrow(all_data),
  nrow(manifest)
))

# Partition columns are automatically added
head(all_data[, .(country, year, reporting_level, hh_id, income)], 3)
```

### Exact Match Filtering (Named List)

Use a named list for exact equality matching (backward compatible):

```{r filter-exact}
# Load only USA data
usa_data <- st_load_parts(
  parts_dir,
  filter = list(country = "USA"),
  as = "dt"
)

cat(sprintf("USA data: %d rows\n", nrow(usa_data)))
unique(usa_data$country)
```

```{r filter-exact-multi}
# Multiple exact matches (AND logic)
usa_2020 <- st_load_parts(
  parts_dir,
  filter = list(country = "USA", year = "2020"),
  as = "dt"
)

cat(sprintf("USA 2020: %d rows\n", nrow(usa_2020)))
unique(usa_2020[, .(country, year)])
```

Notice it is possible to use the numeric string "2020". See [Filter Expression Capabilities](#filter-expression-capabilities)  below for clarification.

### Expression-Based Filtering (Formula)

For flexible filtering with comparisons and boolean logic, use formula syntax:

```{r filter-formula-simple}
# Load data where year > 2021 (numeric comparison)
recent_data <- st_load_parts(
  parts_dir,
  filter = ~ year > 2021,
  as = "dt"
)

cat(sprintf("Recent data (year > 2021): %d rows\n", nrow(recent_data)))
table(recent_data$year)
```

```{r filter-formula-complex}
# Complex boolean logic: (country == "USA" AND year >= 2023) OR (country == "CAN" AND year == 2020)
complex_filter <- st_load_parts(
  parts_dir,
  filter = ~ (country == "USA" & year >= 2023) |
    (country == "CAN" & year == 2020),
  as = "dt"
)

cat(sprintf("Complex filter: %d rows\n", nrow(complex_filter)))
complex_filter[, .N, by = .(country, year)][order(country, year)]
```

```{r filter-in}
# Use %in% for multiple values
selected_countries <- st_load_parts(
  parts_dir,
  filter = ~ country %in% c("USA", "MEX") & year != 2020,
  as = "dt"
)

cat(sprintf("USA/MEX (not 2020): %d rows\n", nrow(selected_countries)))
selected_countries[, .N, by = country]
```

### Filter Expression Capabilities {#filter-expression-capabilities}

Formula filters support:

- **Comparisons**: `>`, `<`, `>=`, `<=`, `==`, `!=`
- **Boolean logic**: `&` (AND), `|` (OR), `!` (NOT)
- **Set operations**: `%in%`, `%nin%` (if defined)
- **Parentheses**: For grouping complex logic
- **Type conversion**: Automatic for numeric partition keys

**Important**: Partition keys are automatically type-converted:
- Numeric strings (`"2020"`) → numeric (`2020`)
- Boolean strings (`"TRUE"`) → logical (`TRUE`)
- Other strings remain strings

## Columnar Loading (Column Selection)

When using parquet or fst formats, you can load only specific columns for massive performance gains:

```{r column-selection}
# Load only income column (+ partition keys automatically included)
income_only <- st_load_parts(
  parts_dir,
  columns = c("income"),
  as = "dt"
)

names(income_only) # income + partition keys (country, year, reporting_level)
```

### Native vs. Fallback Column Selection

| Format   | Native Support | Behavior |
|----------|----------------|----------|
| parquet  | ✅ Yes         | Fast - reads only specified columns from disk |
| fst      | ✅ Yes         | Fast - reads only specified columns from disk |
| qs/qs2   | ❌ No          | Loads full object, then subsets (with warning) |
| rds      | ❌ No          | Loads full object, then subsets (with warning) |
| csv      | ❌ No          | Loads full object, then subsets (with warning) |

```{r column-parquet}
# Parquet: native column selection (efficient)
parquet_cols <- st_load_parts(
  parts_dir,
  columns = c("hh_id", "income"),
  as = "dt"
)

cat("Columns loaded:", paste(names(parquet_cols), collapse = ", "), "\n")
```

### Combining Filters and Column Selection

The real power comes from combining both:

```{r filter-and-columns}
# Load recent USA data, only income and consumption columns
usa_recent_finance <- st_load_parts(
  parts_dir,
  filter = ~ country == "USA" & year >= 2022,
  columns = c("income", "consumption"),
  as = "dt"
)

cat(sprintf(
  "Loaded %d rows × %d columns\n",
  nrow(usa_recent_finance),
  ncol(usa_recent_finance)
))

head(usa_recent_finance, 3)
```

This approach:
1. **Filters partitions** before reading (only loads relevant files)
2. **Reads only needed columns** from those files (with parquet/fst)
3. **Minimizes memory** and I/O for large datasets

## Discovering Partitions with `st_list_parts()`

List available partitions without loading data:

```{r list-parts}
# List all partitions
all_partitions <- st_list_parts(parts_dir)

cat(sprintf("Found %d partitions\n", nrow(all_partitions)))
head(all_partitions[, c("country", "year", "reporting_level")], 6)
```

```{r list-parts-filter}
# List specific partitions with filter
mexico_partitions <- st_list_parts(
  parts_dir,
  filter = ~ country == "MEX" & year >= 2022
)

cat(sprintf("Mexico (2022+): %d partitions\n", nrow(mexico_partitions)))
mexico_partitions[, c("country", "year", "reporting_level")]
```

Use cases:
- **Inventory check**: What partitions exist?
- **Validation**: Ensure expected partitions were created
- **Selective processing**: Get file paths for external processing
- **Metadata extraction**: Extract partition values without loading data

## Single Partition Operations

For fine-grained control, use `st_save_part()` and `st_part_path()`:

```{r single-part}
# Save a single partition explicitly
single_partition <- welfare[
  country == "USA" & year == 2024 & reporting_level == "urban"
]

st_save_part(
  single_partition,
  base = file.path(tdir, "manual_parts"),
  key = list(country = "USA", year = 2024, reporting_level = "urban"),
  code_label = "manual_partition"
)

# Get expected path for a partition
expected_path <- st_part_path(
  base = file.path(tdir, "manual_parts"),
  key = list(country = "USA", year = 2024, reporting_level = "urban")
)

cat("Expected path:\n", expected_path, "\n")
file.exists(expected_path)
```

## Primary Keys and Partitions

Combine partitioning with primary key validation:

```{r pk-validation}
# Define primary key spanning partition columns
pk_manifest <- st_write_parts(
  welfare,
  base = file.path(tdir, "welfare_pk"),
  partitioning = c("country", "year"),
  pk = c("country", "year", "reporting_level", "hh_id"),
  unique = TRUE, # Enforce uniqueness
  .progress = FALSE
)

# Each partition file has PK validation in sidecar metadata
```

When `unique = TRUE`:
- Validates PK uniqueness **within each partition file**
- Stores PK definition in sidecar metadata
- Fails save if duplicates found

## Performance Considerations

### When to Use Partitioning

**Good for:**
- Datasets > 100MB with natural groupings (country, date, category)
- Workloads that frequently filter by partition keys
- Parallel/distributed processing
- Incremental updates (update only changed partitions)

**Not ideal for:**
- Small datasets (< 10MB) - overhead not worth it
- Random access patterns across all partitions
- High cardinality partition keys (millions of unique values)

### Format Selection

| Format  | Write Speed | Read Speed | Column Select | Compression | Use Case |
|---------|-------------|------------|---------------|-------------|----------|
| parquet | Medium      | Fast       | ✅ Yes        | Excellent   | **Default** - best all-around for analytics |
| fst     | Very Fast   | Very Fast  | ✅ Yes        | Good        | High-speed iteration, frequent updates |
| qs2     | Fast        | Fast       | ❌ No         | Excellent   | R-specific objects, complex structures |
| rds     | Slow        | Slow       | ❌ No         | Medium      | Small files, maximum compatibility |

**Recommendation**: Use **parquet** (default) unless you have specific needs.

### Partition Key Selection

Choose partition keys that:

1. **Match your query patterns**: If you always filter by country, make it a partition key
2. **Have moderate cardinality**: 10-1000 unique values ideal per key
3. **Create balanced partitions**: Avoid one huge partition + many tiny ones
4. **Are immutable**: Values shouldn't change over time

**Example - Good:**
```r
partitioning = c("country", "year", "quarter")  # 195 countries × 10 years × 4 quarters = ~8K partitions
```

**Example - Bad:**
```r
partitioning = c("user_id", "timestamp")  # Millions of users × millions of timestamps = too many!
```

### Memory Optimization

```{r memory-optimization}
# Bad: Load everything then filter in R
# all_data <- st_load_parts(parts_dir)
# usa_data <- all_data[country == "USA"]  # ❌ Wasteful

# Good: Filter partitions before loading
usa_data <- st_load_parts(
  parts_dir,
  filter = ~ country == "USA" # ✅ Only loads USA partitions
)

# Better: Also select columns
usa_income <- st_load_parts(
  parts_dir,
  filter = ~ country == "USA",
  columns = c("income") # ✅ Only loads USA partitions, income column
)
```

## Advanced Patterns

### Incremental Updates

Update only specific partitions:

```{r incremental}
# New data for USA 2024 only
new_usa_2024 <- data.table(
  country = "USA",
  year = 2024,
  reporting_level = c("national", "urban"),
  hh_id = 1001:1002,
  income = c(60000, 55000),
  consumption = c(40000, 38000)
)

# Overwrite just the USA 2024 partitions
st_write_parts(
  new_usa_2024,
  base = parts_dir,
  partitioning = c("country", "year", "reporting_level"),
  code_label = "incremental_update",
  .progress = FALSE
)

# Other partitions remain unchanged
```

### Processing Pipeline

```{r pipeline}
# 1. List partitions matching criteria
target_partitions <- st_list_parts(
  parts_dir,
  filter = ~ year >= 2022
)

cat(sprintf("Processing %d partitions...\n", nrow(target_partitions)))

# 2. Load filtered data
recent_data <- st_load_parts(
  parts_dir,
  filter = ~ year >= 2022,
  columns = c("income", "consumption"),
  as = "dt"
)

# 3. Process
recent_data[, income_ratio := income / consumption]

# 4. Save results to new partition set
st_write_parts(
  recent_data,
  base = file.path(tdir, "processed_welfare"),
  partitioning = c("country", "year"),
  code_label = "computed_ratios",
  .progress = FALSE
)
```

## Comparison with Arrow/DuckDB

stamp partitions are compatible with other tools:

```{r arrow-compat, eval=FALSE}
# stamp partitions can be read by Arrow
library(arrow)
ds <- open_dataset(
  parts_dir,
  partitioning = c("country", "year", "reporting_level")
)
ds %>%
  filter(year > 2021) %>%
  select(income, consumption) %>%
  collect()

# Or DuckDB
library(duckdb)
con <- dbConnect(duckdb())
dbGetQuery(
  con,
  sprintf("SELECT * FROM read_parquet('%s/**/*.parquet')", parts_dir)
)
```

Differences:

| Feature | stamp | Arrow | DuckDB |
|---------|-------|-------|--------|
| Format control | ✅ Multiple formats | Parquet focus | Multiple formats |
| Versioning | ✅ Built-in | ❌ None | ❌ None |
| Lineage | ✅ Tracked | ❌ None | ❌ None |
| R integration | ✅ Native | Good | Good |
| Query engine | Basic filtering | Advanced SQL | Full SQL |
| Dependencies | Lightweight | Heavy (C++) | Heavy (C++) |

**Use stamp when:**
- You need versioning and lineage tracking
- You want lightweight dependencies
- You're primarily working in R

**Use Arrow/DuckDB when:**
- You need advanced query capabilities
- You're working with multi-language teams
- Dataset size exceeds memory (lazy evaluation needed)

## Summary

Key takeaways:

1. **`st_write_parts()`**: Auto-partition datasets (eliminates manual loops)
2. **Hive-style paths**: `key1=value1/key2=value2/file.ext` (self-documenting)
3. **Two filter modes**: 
   - Named list: `filter = list(country = "USA")` (exact match)
   - Formula: `filter = ~ year > 2021` (flexible expressions)
4. **Column selection**: `columns = c("col1", "col2")` (native for parquet/fst)
5. **Default format**: Parquet (optimal for analytics)
6. **Partition discovery**: `st_list_parts()` (inventory without loading)

Next steps:
- See `vignette("hashing-and-versions")` for versioning details
- See `vignette("lineage-rebuilds")` for dependency tracking
- See `?st_write_parts` for full API documentation

```{r cleanup, include=FALSE}
# Clean up temp directory
unlink(tdir, recursive = TRUE)
```
